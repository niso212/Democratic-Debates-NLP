import pandas as pd
import numpy as np
import nltk 
import sklearn 

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('reuters')
nltk.download('averaged_perceptron_tagger') 

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.corpus import stopwords

### Functions library

def deep_flatten(xs): 
  '''
  Flattens a deeplist into a list of its base elements
      
  Args: xs = deeplist  
  '''
  
  flat_list = []
  [flat_list.extend(deep_flatten(x)) for x in xs] if isinstance(xs, list) else flat_list.append(xs)
  return flat_list
  
def split_text(text):
  '''
  Splits text by designated mark (:)
     
  Args: text = text to split 
  '''
  count = 0
  text_list = []
  for line in text.copy():
    line = line.split(':')
    text_list.append(line)
    count += 1
  
  return text_list

def split_lines(df):   
  '''
  Tokenizes speaker lines and adds-in new column to the df
  
  Args: df = DataFrame to target  
  '''
  
  line_list = []
  
  for line in df['line']:
    split_line = word_tokenize(str(line))
    split_line = [word for word in split_line if word.isalpha() is True]
    split_line = [word for word in split_line if word not in stopwords.words('english')]
    split_line = [word.lower() for word in split_line]
    line_list.append(split_line) 
    
  return line_list 
  
  
def map_sentiment_value(df):
  count = 0
  val_list = []
  for line in line_list:
    value_map = list(map(sentiment_dict.get, line_list[count]))
    sentiment_val = [val for val in value_map if val != None]
    val_list.append(sentiment_val) 
    count += 1
    
  df['sentiment value'] = val_list 
  
def freq_bar_plot(freq_dist, name): 
  ''' Plots word frequencies from FreqDist operator'''

  fig = plt.figure(figsize=(8,6))
  plt.bar([value[0] for value in freq_dist], [value[1] for value in freq_dist], width=.5, color='royalblue', )
  plt.xticks(rotation=60)
  plt.xlabel('Word')
  plt.ylabel('Frequency')
  plt.title(name)
  plt.show()
  
def find_plot_frequencies(df, names_list):
  for name in names_list: 
    words_list = []
    for line in df.get_group(name)['split line']:
      word = [word for word in line]
      words_list.append(word) 
      words_list = deep_flatten(words_list)
      for word in words_list:
        if word in stopwords.words('english'):
          words_list.remove(word)

      freq_dist = FreqDist(words_list).most_common(20)
      words_set = set(words_list)

    print(name, freq_dist) 
    freq_bar_plot(freq_dist, name) 
    print(name, ':', len(words_set), ':', words_set)
  
def convert_tag(tag):
    '''
    Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets
    '''
    
    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}
    try:
        return tag_dict[tag[0]]
    except KeyError:
        return None
      
def get_synsets(line):
  tokens = word_tokenize(line)
  pos_tags = pos_tag(tokens) 
  tag_list = [tag[1] for tag in pos_tags]
  wordnet_tags = [convert_tag(tag) for tag in tag_list]
  synset_taglist = zip(tokens, wordnet_tags)
  all_synsets = [wn.synsets(x,y) for x,y in synset_taglist]
  top_synset = [val[0] for val in all_synsets if len(val) > 0] 

  return top_synset


def similarity_score(s1, s2):
    """
    Calculate the normalized similarity score of s1 onto s2
    """
    
    syn_scores =[]
    for syn1 in s1:
        score_list = []
        for syn2 in s2:
            score_list.append(syn1.path_similarity(syn2))
        score = [x for x in score_list if x is not None]
        if len(score) > 0 :
            syn_scores.append(max(score))

    return sum(syn_scores)/len(syn_scores)


def document_path_similarity(doc1, doc2):
    """Finds the symmetrical similarity between doc1 and doc2"""

    synsets1 = get_synsets(doc1)
    synsets2 = get_synsets(doc2)

    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2
  

